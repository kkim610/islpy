{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**:  \n",
    "$$p(X) = \\frac{e^{\\beta_0 + \\beta_1x}}{1 + e^{\\beta_0 + \\beta_1x}}$$  \n",
    "\n",
    "so  \n",
    "\n",
    "$$\\frac{p(x)}{1 - p(x)} = \\frac{\\frac{e^{\\beta_0 + \\beta_1x}}{1 + e^{\\beta_0 + \\beta_1x}}}{1 - \\frac{e^{\\beta_0 + \\beta_1x}}{1 + e^{\\beta_0 + \\beta_1x}}}$$  \n",
    "\n",
    "$$= \\frac{\\frac{e^{\\beta_0 + \\beta_1x}}{1 + e^{\\beta_0 + \\beta_1x}}}{\\frac{1+e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}-\\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}}$$  \n",
    "\n",
    "$$= \\frac{\\frac{e^{\\beta_0 + \\beta_1x}}{1 + e^{\\beta_0 + \\beta_1x}}}{\\frac{1}{1+e^{\\beta_0 + \\beta_1x}}}$$  \n",
    "\n",
    "cancelling out the $1+e^{\\beta_0 + \\beta_1x}$ from the top and bottom we arrive at  \n",
    "\n",
    "$$\\frac{p(x)}{1 - p(x)} = e^{\\beta_0 + \\beta_1x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**:  \n",
    "To use the Bayes classifier, we need to find $k$ for which  \n",
    "\n",
    "$$p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\bigl(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\bigr)}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\bigl(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2\\bigr)} = \\frac{\\pi_kexp\\bigl(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\bigr)}{\\sum_{l=1}^K\\pi_lexp\\bigl(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2\\bigr)}$$  \n",
    "\n",
    "is largest. Since the log function is monotonically increasing, this is equivalent to finding $k$ for which  \n",
    "\n",
    "$$\\text{log}(p_k(x)) = -\\frac{1}{2\\sigma^2}(x-\\mu_k)^2 + \\text{log}(\\pi_k) - \\text{log}\\biggl(\\sum_{l=1}^K\\pi_lexp\\bigl(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2\\bigr)\\biggr)$$  \n",
    "\n",
    "is largest. The last term is independent of $k$, so we can limit ourselves in finding $k$ for which  \n",
    "\n",
    "$$-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2 + \\text{log}(\\pi_k) = -\\frac{1}{2\\sigma^2}(x^2-2x\\mu_k+\\mu_k^2)+\\text{log}(\\pi_k)=-x^2\\frac{1}{2\\sigma^2}+x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\text{log}(\\pi_k)$$  \n",
    "\n",
    "is largest. The term containing $x^2$ is independent of $k$, so it remains to find $k$ for which  \n",
    "\n",
    "$$\\delta_k(x) = x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\text{log}(\\pi_k)$$  \n",
    "\n",
    "is largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**:  \n",
    "Proceeding exactly as in (**2**) except without the assumption that $\\sigma_1^2 = \\sigma_2^2 = ... = \\sigma_k^2$, we need to find $k$ for which  \n",
    "\n",
    "$$p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma_k}exp\\bigl(-\\frac{1}{2\\sigma_k^2}(x - \\mu_k)^2\\bigr)}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma_l}exp\\bigl(-\\frac{1}{2\\sigma_l^2}(x-\\mu_l)^2\\bigr)} = \\frac{\\pi_k\\frac{1}{\\sigma_k}exp\\bigl(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\bigr)}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sigma_l}exp\\bigl(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2\\bigr)}$$  \n",
    "\n",
    "is largest.  Taking the log of both sides we get  \n",
    "\n",
    "$$\\log{(p_k(x))}=-\\frac{1}{2\\sigma_k^2}(x-\\mu_k)^2+\\log{(\\pi_k)}-log{(\\sigma_k)}-\\log{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sigma_l}exp\\bigl(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2\\bigr)}$$  \n",
    "\n",
    "The last term is independent of $k$, so we can limit ourselves to finding $k$ for which  \n",
    "\n",
    "$$\\delta_k(x)=-x^2\\frac{1}{2\\sigma_k^2}+x\\frac{\\mu_k}{\\sigma_k}-\\frac{\\mu_k^2}{2\\sigma_k^2}+\\log{(\\pi_k)}-\\log{(\\sigma_k)}$$  \n",
    "\n",
    "is largest. From the first term we can clearly see that the Bayes classifier in this case is in fact quadratic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 (a)**:  \n",
    "If $X$ is uniformly distributed on [0, 1], then observations in the range [0.55, 0.65] will represent, on average, 10% (0.1) of the available observations (i.e. $\\frac{0.65 - 0.55}{1 - 0}$). *Note this does not take into account observations > 0.95 or < 0.05, which will make the percentage a bit smaller.*  \n",
    "\n",
    "**4 (b)**:  \n",
    "Since the range intervals for both $X_1$ and $X_2$ are 0.1 (10%), the fraction of available data that would be used would be $0.1^2=0.01$ (1%)\n",
    "\n",
    "**4 (c)**:  \n",
    "Similar to the previous problem, the range intervals for all predictors would be 0.1 (10%). For 100 predictors, the percentage of available data used for the prediction would be $0.1^{100} = 1e^{-100}=1e^{-98}\\%$.  \n",
    "\n",
    "**4 (d)**:  \n",
    "Suppose we have 10,000 observations. If $p=1$ then, on average, 1,000 observations would be used for prediction, 100 for $p=2$, 10 for $p=3$, and 1 for $p=4$. On average there would be zero observations that fit the criteria if $p>4$. Stated another way, the fraction of observations that would be used for a prediction would be $(0.1)^p$.  \n",
    "\n",
    "**4 (e)**:  \n",
    "For $p=1$ we have a line of length 0.1. For $p=2$ we have a square with area 0.1, thus the sides are of length $0.1^{1/2}$. For $p=3$ we have a cube with a volume of 0.1, thus the sides are of length $0.1^{1/3}$. For $p=100$ the hypercube has sides of length $0.1^{1/100}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**5 (a)**:  \n",
    "Because the QDA is more flexible, we expect it to perform better on the training set. However if the Bayes decision boundary is linear, then we expect LDA to perform better on the unseen test data.  \n",
    "\n",
    "**5 (b)**:  \n",
    "Again, we expect QDA to perform better on the training set due to it's increased flexibility over LDA. If the Bayes decision boundary is non-linear, we expect QDA to perform better on the test set than LDA.  \n",
    "\n",
    "**5 (c)**:  \n",
    "As the sample size increases we expect the test prediction accuracy of QDA to improve relative to LDA. This is because the more flexible model, QDA, has higher variance relative to LDA. With increasing sample size, model variance of QDA becomes less of a concern.  \n",
    "\n",
    "**5 (d)**:  \n",
    "*False*. With fewer sample points, the variance for the more flexible model such as QDA, may lead to overfit. This would cause poor performance on the test set relative to LDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6 (a)**:  \n",
    "\n",
    "$$\\widehat{p}(X) = \\frac{e^{\\widehat{\\beta}_0 + \\widehat{\\beta}_1X_1 + \\widehat{\\beta}_2X_2}}{1 + e^{\\widehat{\\beta}_0 + \\widehat{\\beta}_1X_1 + \\widehat{\\beta}_2X_2}}=\\frac{e^{-6 + 0.05\\times40 + 3.5}}{1+e^{-6 + 0.05\\times40 + 3.5}}=0.3775=37.75\\%$$  \n",
    "\n",
    "**6 (b)**:  \n",
    "\n",
    "$$0.5=\\frac{e^{-2.5 + 0.05\\times X_1}}{1+e^{-2.5 + 0.05\\times X_1}}$$  \n",
    "\n",
    "$$1+e^{-2.5 + 0.05\\times X_1}=2e^{-2.5 + 0.05\\times X_1}$$  \n",
    "\n",
    "$$1 = e^{-2.5 + 0.05\\times X_1}$$  \n",
    "\n",
    "$$log(1) = -2.5 + 0.05X_1$$  \n",
    "\n",
    "$$2.5=0.05X_1$$  \n",
    "\n",
    "$$X_1 = 50\\text{ hours}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7**:  \n",
    "\n",
    "Given $\\mu_{yes}=10$, $\\mu_{no}=0$, $\\pi_{yes}=0.8$, $\\pi_{no}=0.2$, and $\\sigma_{yes}^2=\\sigma_{no}^2=36$ this equation  \n",
    "\n",
    "$$p_k(X) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2)}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2)}=\\frac{\\pi_kexp(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2)}{\\sum_{l=1}^K\\pi_lexp(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2)}$$  \n",
    "\n",
    "becomes  \n",
    "\n",
    "$$p_{yes}(X)=\\frac{\\pi_{yes}exp(-\\frac{1}{2\\sigma^2}(x-\\mu_{yes})^2)}{\\sum_{l=1}^K\\pi_lexp(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2)}$$  \n",
    "\n",
    "$$=\\frac{\\pi_{yes}exp(-\\frac{1}{2\\sigma^2}(x-\\mu_{yes})^2)}{\\pi_{yes}exp(-\\frac{1}{2\\sigma^2}(x-\\mu_{yes})^2)+\\pi_{no}exp(-\\frac{1}{2\\sigma^2}(x-\\mu_{no})^2)}$$\n",
    "\n",
    "$$=\\frac{0.8exp(-\\frac{1}{2*36}(x-10)^2)}{0.8exp(-\\frac{1}{2*36}(x-10)^2)+0.2exp(-\\frac{1}{2*36}x^2)}$$  \n",
    "\n",
    "$$p_{yes}(X=4)=\\frac{0.8exp(-\\frac{1}{2*36}(4-10)^2)}{0.8exp(-\\frac{1}{2*36}(4-10)^2)+0.2exp(-\\frac{1}{2*36}4^2)}$$  \n",
    "\n",
    "$$p_{yes}(X=4)=0.7519=75.19\\%$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
