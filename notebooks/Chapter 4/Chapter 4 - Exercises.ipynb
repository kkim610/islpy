{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**:  \n",
    "$$p(X) = \\frac{e^{\\beta_0 + \\beta_1x}}{1 + e^{\\beta_0 + \\beta_1x}}$$  \n",
    "\n",
    "so  \n",
    "\n",
    "$$\\frac{p(x)}{1 - p(x)} = \\frac{\\frac{e^{\\beta_0 + \\beta_1x}}{1 + e^{\\beta_0 + \\beta_1x}}}{1 - \\frac{e^{\\beta_0 + \\beta_1x}}{1 + e^{\\beta_0 + \\beta_1x}}}$$  \n",
    "\n",
    "$$= \\frac{\\frac{e^{\\beta_0 + \\beta_1x}}{1 + e^{\\beta_0 + \\beta_1x}}}{\\frac{1+e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}-\\frac{e^{\\beta_0 + \\beta_1x}}{1+e^{\\beta_0 + \\beta_1x}}}$$  \n",
    "\n",
    "$$= \\frac{\\frac{e^{\\beta_0 + \\beta_1x}}{1 + e^{\\beta_0 + \\beta_1x}}}{\\frac{1}{1+e^{\\beta_0 + \\beta_1x}}}$$  \n",
    "\n",
    "cancelling out the $1+e^{\\beta_0 + \\beta_1x}$ from the top and bottom we arrive at  \n",
    "\n",
    "$$\\frac{p(x)}{1 - p(x)} = e^{\\beta_0 + \\beta_1x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**:  \n",
    "To use the Bayes classifier, we need to find $k$ for which  \n",
    "\n",
    "$$p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\bigl(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\bigr)}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\bigl(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2\\bigr)} = \\frac{\\pi_kexp\\bigl(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\bigr)}{\\sum_{l=1}^K\\pi_lexp\\bigl(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2\\bigr)}$$  \n",
    "\n",
    "is largest. Since the log function is monotonically increasing, this is equivalent to finding $k$ for which  \n",
    "\n",
    "$$\\text{log}(p_k(x)) = -\\frac{1}{2\\sigma^2}(x-\\mu_k)^2 + \\text{log}(\\pi_k) - \\text{log}\\biggl(\\sum_{l=1}^K\\pi_lexp\\bigl(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2\\bigr)\\biggr)$$  \n",
    "\n",
    "is largest. The last term is independent of $k$, so we can limit ourselves in finding $k$ for which  \n",
    "\n",
    "$$-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2 + \\text{log}(\\pi_k) = -\\frac{1}{2\\sigma^2}(x^2-2x\\mu_k+\\mu_k^2)+\\text{log}(\\pi_k)=-x^2\\frac{1}{2\\sigma^2}+x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\text{log}(\\pi_k)$$  \n",
    "\n",
    "is largest. The term containing $x^2$ is independent of $k$, so it remains to find $k$ for which  \n",
    "\n",
    "$$\\delta_k(x) = x\\frac{\\mu_k}{\\sigma^2}-\\frac{\\mu_k^2}{2\\sigma^2}+\\text{log}(\\pi_k)$$  \n",
    "\n",
    "is largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**:  \n",
    "Proceeding exactly as in (**2**) except without the assumption that $\\sigma_1^2 = \\sigma_2^2 = ... = \\sigma_k^2$, we need to find $k$ for which  \n",
    "\n",
    "$$p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma_k}exp\\bigl(-\\frac{1}{2\\sigma_k^2}(x - \\mu_k)^2\\bigr)}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma_l}exp\\bigl(-\\frac{1}{2\\sigma_l^2}(x-\\mu_l)^2\\bigr)} = \\frac{\\pi_k\\frac{1}{\\sigma_k}exp\\bigl(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\bigr)}{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sigma_l}exp\\bigl(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2\\bigr)}$$  \n",
    "\n",
    "is largest.  Taking the log of both sides we get  \n",
    "\n",
    "$$\\log{(p_k(x))}=-\\frac{1}{2\\sigma_k^2}(x-\\mu_k)^2+\\log{(\\pi_k)}-log{(\\sigma_k)}-\\log{\\sum_{l=1}^K\\pi_l\\frac{1}{\\sigma_l}exp\\bigl(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2\\bigr)}$$  \n",
    "\n",
    "The last term is independent of $k$, so we can limit ourselves to finding $k$ for which  \n",
    "\n",
    "$$\\delta_k(x)=-x^2\\frac{1}{2\\sigma_k^2}+x\\frac{\\mu_k}{\\sigma_k}-\\frac{\\mu_k^2}{2\\sigma_k^2}+\\log{(\\pi_k)}-\\log{(\\sigma_k)}$$  \n",
    "\n",
    "is largest. From the first term we can clearly see that the Bayes classifier in this case is in fact quadratic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 (a)**:  \n",
    "If $X$ is uniformly distributed on [0, 1], then observations in the range [0.55, 0.65] will represent, on average, 10% (0.1) of the available observations (i.e. $\\frac{0.65 - 0.55}{1 - 0}$). *Note this does not take into account observations > 0.95 or < 0.05, which will make the percentage a bit smaller.*  \n",
    "\n",
    "**4 (b)**:  \n",
    "Since the range intervals for both $X_1$ and $X_2$ are 0.1 (10%), the fraction of available data that would be used would be $0.1^2=0.01$ (1%)\n",
    "\n",
    "**4 (c)**:  \n",
    "Similar to the previous problem, the range intervals for all predictors would be 0.1 (10%). For 100 predictors, the percentage of available data used for the prediction would be $0.1^{100} = 1e^{-100}=1e^{-98}\\%$.  \n",
    "\n",
    "**4 (d)**:  \n",
    "Suppose we have 10,000 observations. If $p=1$ then, on average, 1,000 observations would be used for prediction, 100 for $p=2$, 10 for $p=3$, and 1 for $p=4$. On average there would be zero observations that fit the criteria if $p>4$. Stated another way, the fraction of observations that would be used for a prediction would be $(0.1)^p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
