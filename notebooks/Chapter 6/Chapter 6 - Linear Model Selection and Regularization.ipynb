{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regression setting, the standard linear model  \n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p + \\epsilon$$  \n",
    "\n",
    "is commonly used to describe the relationship between a response $Y$ and a set of variables $X_1, X_2, ..., X_p$. In this chapter we will discuss some ways in which the simple linear model can be improved, by replacing plain least squares fitting with some alternative fitting procedures.  \n",
    "\n",
    "Why would we want to use another fitting procedure instead of least squares?  \n",
    " 1. *Prediction Accuracy*  \n",
    "  * Provided that the true relationship between the response and the predictors is approximately linear, the least squares estimates will have low bias. If $n\\gg p$, then the least squares estimates tend to also have low variance, and will perform well on test observations. However if $n$ is not much larger than $p$, then there can be a lot of variability in the least squares fit, resulting in overfitting and poor predictions on future observations. And if $p > n$, then there is no longer a unique least squares coefficient estimate (The variance is *infinite* so the method cannot be used at all). By *constraining* or *shrinking* the estimated coefficients, we can ofen substantially reduce the variance at the cost of a negligible increase in bias, which can lead to substantial improvements in the accuracy of predictions.  \n",
    "  <br>\n",
    " 2. *Model Interpretability*\n",
    "  * In many cases, some or many of the variables are not actually associated with the response. Including such *irrelevant* variables leads to unnecesary complexity in the resulting model. By removing these variables (setting their coefficients to zero), we can obtain a model that is more easily interpreted. In this case, least squares is extremely unlikely to yield any coefficient estimates that are exactly zero.  \n",
    "  \n",
    "In this chapter we discuss three important classes of alternatives to using least squares fit:  \n",
    "\n",
    " 1. *Subset Selection*  \n",
    "  * Identify a subset of the $p$ predictors that we believe are related to the response, then fit a model using least squares on the reduced set of variables.  \n",
    "  <br>\n",
    " 2. *Shrinkage*  \n",
    "  * Fit a model with all $p$ predictors but shrink coefficients towards zero relative to the least squares estimates. This shrinkage (a.k.a. *regularization*) has the effect of reducing variance. Depending on what type of shrinkage is performed, some of the coefficients may be estimated to be exactly zero. Hence, shrinkage methods can also perform variable selection.  \n",
    "  <br>\n",
    " 3. *Dimension Reduction*  \n",
    "  * This approach involves *projecting* the $p$ predictors into a $M$-dimensional space, where $M<p$. This is achieved by computing $M$ different *linear combinations* or *projections* of the variables. Then these $M$ projections are used as predictors to fit a linear regression model by least squares.  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Subset Selection  \n",
    "To perform *best subset selection* we fit a separate least squares regression for each possible combination of the $p$ predictors and then identify which one is the *best*. The problem of selecting the *best model* from the $2^p$ possibilities considered by best subset selection is not trivial. This is usually broken up into two stages, as in Algorithm 6.1.  \n",
    "\n",
    "---\n",
    "**Algorithm 6.1**: *Best subset selection*  \n",
    "\n",
    "  1. Let $M_0$ denote the *null model*, which contains no predictors. This model simply predicts the sample mean for each observation.  \n",
    "  <br>\n",
    "  2. For $k=1, 2, ..., p$:  \n",
    "    a. Fit all $\\bigl(_k^p\\bigr)$ models that contain exactly $k$ predictions  \n",
    "    b. Pick the best among these $\\bigl(_k^p\\bigr)$ models and call it $M_k$. Here *best* is defined as having the smallest RSS, or equivalently largest $R^2$.  \n",
    "  <br>\n",
    "  3. Select a single best model from among $M_0, ..., M_p$ using cross validated prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$.\n",
    "\n",
    "---  \n",
    "\n",
    "In Algorithm 6.1, step 2 identifies the best model for each subset size (on the training data) which reduces the problem from one of $2^p$ possible models to one of $p+1$ possible models.  \n",
    "\n",
    "Now in order to select a single best model, we must simple choose among these $p+1$ options. However, since RSS decreases monotonically and $R^2$ increases monotonically as the number of features increases, we will always end up with a model involving all the variables. The problem is that a low RSS and a high $R^2$ indicates a model with a low *training* error, whereas we wish to choose a model that has a low *test* error. Therefore, in step 3, we us cross-validated prediction error, $C_p$, BIC, or adjusted $R^2$ in order to select among $M_0, M_1, ..., M_p$.  \n",
    "\n",
    "In the case of logistic regression, *deviance* plays the role of RSS in step 2 for selecting the best model. Deviance is negative two times the maximized log-likelihood. The smaller the deviance, the better the fit.  \n",
    "\n",
    "While best subset selection is simple and conceptually appealing, it suffers from computational limitations. If $p=10$ there are approximately 1,000 possible models. If $p=20$ there are over one million possibilities. Best subset becomes computationally infeasible around $p=40$, even with extremely fast modern computers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise Selection  \n",
    "Best subset selection cannot be applied with very large $p$. Best subset selection may also suffer from statistical problems when $p$ is large. The larger the search space, the higher the chance of finding models that look good on training data, even though they might not have any predictive power on future data. Thus, an enourmous search space can lead to overfitting and high variance of the coefficient estimates.  \n",
    "\n",
    "*Stepwise* methods explore a far more restricted set of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Stepwise Selection  \n",
    "*Forward stepwise selection* begins with a model containing no predictors, and then adds predictors to the model that gives the greatest *additional* improvement, one-at-a-time until all predictors are in the model.  \n",
    "\n",
    "---\n",
    "**Algorithm 6.2**: *Forward stepwise selection  \n",
    "\n",
    "  1. Let $M_0$ denote the *null* model, which contains no predictors.  \n",
    "  <br>\n",
    "  2. For $k=0, ..., p-1$:  \n",
    "    a. Consider all $p-k$ models that augment the predictors in $M_k$ with one additional predictor.  \n",
    "    b. Choose the *best* among these $p-k$ models, and call it $M_{k+1}$. Here *best* is defined as having the smallest RSS or highest $R^2$.  \n",
    "  <br>\n",
    "  3. Select a single best model from among $M_0, ..., M_p$ using cross validated prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$  \n",
    "  \n",
    "---  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
